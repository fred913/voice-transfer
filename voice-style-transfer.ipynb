{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/fred913/voice-style-transfering-based-on-random-cnn?scriptVersionId=87020649\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"This project is based on [randomCNN-voice-transfer](https://github.com/mazzzystar/randomCNN-voice-transfer).\n\n`input1.wav` is a set of voice that was said by *Viator* and `input2.wav` is a set of voice that was said by *Viatrix*.\n\n*Viator* CV: [鹿喑](https://space.bilibili.com/302145)\n\n*Viatrix* CV: [宴宁](https://space.bilibili.com/14897804)\n\nThe `mv1.wav` and the `mv2.wav` are vocal sound files of the song *Let The Wind Tell You (my cover version)*. \n\n**NOTICE**: the files input1 and input2 are NOT authorized to use in this project. I'm also just a fan of these characters. \n\n\n- 词曲/制作人 Lyricist / Composer / Producer ：ChiliChill \n- 编曲 Arrangement：ChiliChill \n- 贝斯 Bass：冯子明、山口進也 \n- 长笛 Flute：Salit Lahav \n- 弦乐编写 String music：胡静成 \n- 小提琴 Violin：庞阔 / 张浩 \n- 中提琴 Viola：毕芳 \n- 大提琴 Violoncello：郎莹 \n- 弦乐录音 String Music Recording：李昕达@九紫天诚\n- ChiliChill is a two person independent music group composed of members Yu H. and CuSummer\n- ChiliChill是由成员Yu H.和CuSummer组成的二人独立音乐团体\n- 人声/混音 Vocal/Mixing：剩饭不吃剩饭/fred913 (me)\n**NOTICE**: the official music is mixed by ChiliChill. In this project I'm using my cover version of this music. \n\n## Preparations\n - Get the voice files of one charactor. \n - Get the voice file of the voice content you want. \n - Upload a dataset with the content and style inside. \n\n## Recommended Voice Sources\n - Games (e.x. recording from the `voices` page in Genshin Impact)\n\n# Risks\n - 请务必重视该项目的法律风险。我不提倡且不支持任何非法的伪造身份的行为。该项目拥有更好的应用场景。而不应被某些人糟蹋，导致该项目被所有人唾弃。\n - Please pay **attention** to the **legal** risks of the project. I **do not** advocate and support any illegal identity forgery. The project has better application scenarios. It **should not be spoiled** by some people, resulting in the project being **despised**.\n - 去你妈的骗子，别来用这个项目瞎折腾骗人。滚一边去，傻逼（指骗子，请不要对号入座。）\n\n# How to run\n1. First, you should configure the project HERE","metadata":{}},{"cell_type":"code","source":"DATASET_NAME=\"rcnntmp1\"  # Put the name of the dataset (or the folder) here.\nCONTENT_FILENAME = \"mv2.wav\"\nSTYLE_FILENAME = \"input2.wav\"\n\n## If you're running this project somewhere else, change this/these option(s):\nDATASET_ACCESS = \"/kaggle/input/%s/%s\"  # upload a dataset with the audio files inside (Kaggle)\n# DATASET_ACCESS = \"./%s/%s\"  # just create a folder with the audio files in the same folder as this notebook (local)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:03:38.718488Z","iopub.execute_input":"2022-02-04T14:03:38.718901Z","iopub.status.idle":"2022-02-04T14:03:38.723824Z","shell.execute_reply.started":"2022-02-04T14:03:38.718842Z","shell.execute_reply":"2022-02-04T14:03:38.722756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. And then, click `Save Version` on the right top of the screen. Don't forget to turn on the *GPU*! \n3. Wait for 10-20 hours.\n4. Well done! Download the `output.wav` in the *results*.","metadata":{}},{"cell_type":"code","source":"import matplotlib\nmatplotlib.use('agg')\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\nimport time\nimport math\nimport argparse\nimport gc\nimport tqdm\nimport os\nimport torch\nimport torch.nn as nn\nimport librosa\nimport numpy as np\nimport soundfile\nfrom packaging import version","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cuda = True if torch.cuda.is_available() else False\n\nN_FFT = 512\nN_CHANNELS = round(1 + N_FFT/2)\nOUT_CHANNELS = 32\n\n\nclass RandomCNN(nn.Module):\n    def __init__(self):\n        super(RandomCNN, self).__init__()\n\n        # 2-D CNN\n        self.conv1 = nn.Conv2d(1, OUT_CHANNELS, kernel_size=(3, 1), stride=1, padding=0)\n        self.LeakyReLU = nn.LeakyReLU(0.2)\n\n        # Set the random parameters to be constant.\n        weight = torch.randn(self.conv1.weight.data.shape)\n        self.conv1.weight = torch.nn.Parameter(weight, requires_grad=False)\n        bias = torch.zeros(self.conv1.bias.data.shape)\n        self.conv1.bias = torch.nn.Parameter(bias, requires_grad=False)\n\n    def forward(self, x_delta):\n        out = self.LeakyReLU(self.conv1(x_delta))\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:03:38.725625Z","iopub.execute_input":"2022-02-04T14:03:38.726126Z","iopub.status.idle":"2022-02-04T14:03:38.736643Z","shell.execute_reply.started":"2022-02-04T14:03:38.726078Z","shell.execute_reply":"2022-02-04T14:03:38.736073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def librosa_write(outfile, x, sr):\n    if version.parse(librosa.__version__) < version.parse('0.8.0'):\n        librosa.output.write_wav(outfile, x, sr)\n    else:\n        soundfile.write(outfile, x, sr)\n\ndef wav2spectrum(filename):\n    x, sr = librosa.load(filename)\n    S = librosa.stft(x, N_FFT)\n    p = np.angle(S)\n\n    S = np.log1p(np.abs(S))\n    return S, sr\n\n\ndef spectrum2wav(spectrum, sr, outfile):\n    # Return the all-zero vector with the same shape of `a_content`\n    a = np.exp(spectrum) - 1\n    p = 2 * np.pi * np.random.random_sample(spectrum.shape) - np.pi\n    for i in range(50):\n        S = a * np.exp(1j * p)\n        x = librosa.istft(S)\n        p = np.angle(librosa.stft(x, N_FFT))\n    librosa_write(outfile, x, sr)\n\n\ndef wav2spectrum_keep_phase(filename):\n    x, sr = librosa.load(filename)\n    S = librosa.stft(x, N_FFT)\n    p = np.angle(S)\n\n    S = np.log1p(np.abs(S))\n    return S, p, sr\n\n\ndef spectrum2wav_keep_phase(spectrum, p, sr, outfile):\n    # Return the all-zero vector with the same shape of `a_content`\n    a = np.exp(spectrum) - 1\n    for i in range(50):\n        S = a * np.exp(1j * p)\n        x = librosa.istft(S)\n        p = np.angle(librosa.stft(x, N_FFT))\n    librosa_write(outfile, x, sr)\n\n\ndef compute_content_loss(a_C, a_G):\n    \"\"\"\n    Compute the content cost\n\n    Arguments:\n    a_C -- tensor of dimension (1, n_C, n_H, n_W)\n    a_G -- tensor of dimension (1, n_C, n_H, n_W)\n\n    Returns:\n    J_content -- scalar that you compute using equation 1 above\n    \"\"\"\n    m, n_C, n_H, n_W = a_G.shape\n\n    # Reshape a_C and a_G to the (m * n_C, n_H * n_W)\n    a_C_unrolled = a_C.view(m * n_C, n_H * n_W)\n    a_G_unrolled = a_G.view(m * n_C, n_H * n_W)\n\n    # Compute the cost\n    J_content = 1.0 / (4 * m * n_C * n_H * n_W) * torch.sum((a_C_unrolled - a_G_unrolled) ** 2)\n\n    return J_content\n\n\ndef gram(A):\n    \"\"\"\n    Argument:\n    A -- matrix of shape (n_C, n_L)\n\n    Returns:\n    GA -- Gram matrix of shape (n_C, n_C)\n    \"\"\"\n    GA = torch.matmul(A, A.t())\n\n    return GA\n\n\ndef gram_over_time_axis(A):\n    \"\"\"\n    Argument:\n    A -- matrix of shape (1, n_C, n_H, n_W)\n\n    Returns:\n    GA -- Gram matrix of A along time axis, of shape (n_C, n_C)\n    \"\"\"\n    m, n_C, n_H, n_W = A.shape\n\n    # Reshape the matrix to the shape of (n_C, n_L)\n    # Reshape a_C and a_G to the (m * n_C, n_H * n_W)\n    A_unrolled = A.view(m * n_C * n_H, n_W)\n    GA = torch.matmul(A_unrolled, A_unrolled.t())\n\n    return GA\n\n\ndef compute_layer_style_loss(a_S, a_G):\n    \"\"\"\n    Arguments:\n    a_S -- tensor of dimension (1, n_C, n_H, n_W)\n    a_G -- tensor of dimension (1, n_C, n_H, n_W)\n\n    Returns:\n    J_style_layer -- tensor representing a scalar style cost.\n    \"\"\"\n    m, n_C, n_H, n_W = a_G.shape\n\n    # Reshape the matrix to the shape of (n_C, n_L)\n    # Reshape a_C and a_G to the (m * n_C, n_H * n_W)\n\n    # Calculate the gram\n    # !!!!!! IMPORTANT !!!!! Here we compute the Gram along n_C,\n    # not along n_H * n_W. But is the result the same? No.\n    GS = gram_over_time_axis(a_S)\n    GG = gram_over_time_axis(a_G)\n\n    # Computing the loss\n    J_style_layer = 1.0 / (4 * (n_C ** 2) * (n_H * n_W)) * torch.sum((GS - GG) ** 2)\n\n    return J_style_layer\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:03:38.73823Z","iopub.execute_input":"2022-02-04T14:03:38.738737Z","iopub.status.idle":"2022-02-04T14:03:38.760332Z","shell.execute_reply.started":"2022-02-04T14:03:38.738673Z","shell.execute_reply":"2022-02-04T14:03:38.759508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ncuda = True if torch.cuda.is_available() else False\nif not cuda:\n    print(\n        \"NOTICE: Cuda is NOT available. Training with CPU will DEEPLY decrease the training speed. \"\n    )\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-content', help='Content input', default=DATASET_ACCESS % (DATASET_NAME, CONTENT_FILENAME))\nparser.add_argument('-content_weight',\n                    help='Content weight. Default is 1e2',\n                    default=1e2)\nparser.add_argument('-style', help='Style input', default=DATASET_ACCESS % (DATASET_NAME, STYLE_FILENAME))\nparser.add_argument('-style_weight',\n                    help='Style weight. Default is 1',\n                    default=1)\nparser.add_argument('-epochs',\n                    type=int,\n                    help='Number of epoch iterations. Default is 20000',\n                    default=20000)\nparser.add_argument('-print_interval',\n                    type=int,\n                    help='Number of epoch iterations between printing losses',\n                    default=1000)\nparser.add_argument('-plot_interval',\n                    type=int,\n                    help='Number of epoch iterations between plot points',\n                    default=1000)\nparser.add_argument('-learning_rate', type=float, default=0.002)\nparser.add_argument('-output',\n                    help='Output file name. Default is \"output\"',\n                    default='output')\nargs = parser.parse_args(args=())\n\nCONTENT_FILENAME = args.content\nSTYLE_FILENAME = args.style\n\na_content, sr = wav2spectrum(CONTENT_FILENAME)\na_style, sr = wav2spectrum(STYLE_FILENAME)\n\na_content_torch = torch.from_numpy(a_content)[None, None, :, :]\nif cuda:\n    a_content_torch = a_content_torch.cuda()\nprint(a_content_torch.shape)\na_style_torch = torch.from_numpy(a_style)[None, None, :, :]\nif cuda:\n    a_style_torch = a_style_torch.cuda()\nprint(a_style_torch.shape)\n\nmodel = RandomCNN()\nmodel.eval()\n\na_C_var = Variable(a_content_torch, requires_grad=False).float()\na_S_var = Variable(a_style_torch, requires_grad=False).float()\nif cuda:\n    model = model.cuda()\n    a_C_var = a_C_var.cuda()\n    a_S_var = a_S_var.cuda()\n\na_C = model(a_C_var)\na_S = model(a_S_var)\n\n# Optimizer\nlearning_rate = args.learning_rate\na_G_var = Variable(torch.randn(a_content_torch.shape) * 1e-3)\nif cuda:\n    a_G_var = a_G_var.cuda()\na_G_var.requires_grad = True\noptimizer = torch.optim.Adam([a_G_var])\n\n# coefficient of content and style\nstyle_param = args.style_weight\ncontent_param = args.content_weight\n\nnum_epochs = args.epochs\nprint_every = args.print_interval\nplot_every = args.plot_interval\n\n# Keep track of losses for plotting\ncurrent_loss = 0\nall_losses = []\n\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\nstart = time.time()\n# Train the Model\nbar = tqdm.tqdm(range(1, num_epochs + 1))\nfor epoch in bar:\n    gc.collect()\n    torch.cuda.empty_cache()\n    optimizer.zero_grad()\n    a_G = model(a_G_var)\n\n    content_loss = content_param * compute_content_loss(a_C, a_G)\n    style_loss = style_param * compute_layer_style_loss(a_S, a_G)\n    loss = content_loss + style_loss\n    loss.backward()\n    optimizer.step()\n\n    bar.set_description_str(\n        \"content_loss={:4f}, style_loss={:4f}, total_loss={:4f}\".format(\n            content_loss.item(), style_loss.item(), loss.item()))\n    current_loss += loss.item()\n\n    # Add current loss avg to list of losses\n    if epoch % plot_every == 0:\n        all_losses.append(current_loss / plot_every)\n        current_loss = 0\n\n    if os.path.isfile(\"get_temp_file.flag\"):\n        print(\"output one file\")\n        gen_spectrum = a_G_var.cpu().data.numpy().squeeze()\n        gen_audio_C = \"temp_opt.wav\"\n        spectrum2wav(gen_spectrum, sr, gen_audio_C)\n        os.remove(\"get_temp_file.flag\")\n\ngen_spectrum = a_G_var.cpu().data.numpy().squeeze()\ngen_audio_C = args.output + \".wav\"\nspectrum2wav(gen_spectrum, sr, gen_audio_C)\n\nplt.figure()\nplt.plot(all_losses)\nplt.savefig('loss_curve.png')\n\nplt.figure(figsize=(5, 5))\n# we then use the 2nd column.\nplt.subplot(1, 1, 1)\nplt.title(\"Content Spectrum\")\nplt.imsave('Content_Spectrum.png', a_content[:400, :])\n\nplt.figure(figsize=(5, 5))\n# we then use the 2nd column.\nplt.subplot(1, 1, 1)\nplt.title(\"Style Spectrum\")\nplt.imsave('Style_Spectrum.png', a_style[:400, :])\n\nplt.figure(figsize=(5, 5))\n# we then use the 2nd column.\nplt.subplot(1, 1, 1)\nplt.title(\"CNN Voice Transfer Result\")\nplt.imsave('Gen_Spectrum.png', gen_spectrum[:400, :])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-04T14:03:38.836837Z","iopub.execute_input":"2022-02-04T14:03:38.837096Z","iopub.status.idle":"2022-02-04T14:04:41.185111Z","shell.execute_reply.started":"2022-02-04T14:03:38.837068Z","shell.execute_reply":"2022-02-04T14:04:41.183956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}